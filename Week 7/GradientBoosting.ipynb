{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientBoosting.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "-lW-fRdhI9Mv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import time\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#лайфхак, если не тянет компьютер - для расчета можно использовать Google Colab, в моем случае помогло ускориться раз в 5\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "# 1. Загрузим наш датасет: в X запишем признаки, не связанные с итогами матча, в y - исходы матчей\n",
        "data = pd.read_csv('./features.csv', index_col='match_id')\n",
        "X = pd.DataFrame(data.loc[:, 'start_time':'dire_first_ward_time'])\n",
        "y = pd.DataFrame(data['radiant_win'])\n",
        "\n",
        "# 2. Узнаем столбцы, в которых есть пропуски и запишем их для удобства в массив nan_columns\n",
        "nan_columns = np.array([])\n",
        "\n",
        "for col in data.columns:\n",
        "  if np.isnan(data[col]).any() == True:\n",
        "    nan_columns = np.append(nan_columns, col)\n",
        "    #print(col, data[col].count()) #для просмотра количества непустых значений\n",
        "    \n",
        "#Распечатаем наш массив с колонками, содержащими NaN значения, выпишем описания двух признаков\n",
        "#print(nan_columns)\n",
        "#first_blood_time - игровое время первой крови\n",
        "#first_blood_team - команда, совершившая первую кровь\n",
        "#обоснование для пропущенных значений - события не наступили в течении первых 5 минут\n",
        "\n",
        "# 3. NaN значения можно заполнить пропущенные значения разными способами:\n",
        "#    1: заполнить нулями, чтобы данные пропуски не вносили вклад в логистической регрессии\n",
        "#    2: заполнить значениями, как будто они наступили на 5 минуте, что соответсвует значению 300 (= 60*5)\n",
        "#    3: заполнить средними значениями\n",
        "# Попробуем способ 2:\n",
        "X = X.fillna(300)\n",
        "\n",
        "# 4. Столбец с целевой переменной - 'radiant_win', мы его загрузили в y\n",
        "\n",
        "# 5.1 Выберем разбиение:\n",
        "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
        "\n",
        "# 5.2 Обучим наш классификатор и проверим качество на кросс-валидации\n",
        "# Ниже для простоты приведен код, который обучается на фиксированном значении количества деревьев и скорости обучения\n",
        "for p in [0.2]:\n",
        "  clf = GradientBoostingClassifier(n_estimators=500, verbose=False, random_state=42, learning_rate=p)\n",
        "  clf.fit(X, y)\n",
        "  c_v = cross_val_score(clf, X, y, cv=kf, scoring='roc_auc')\n",
        "  print('{c} learning_rate = {p}'.format(c = round(c_v.mean(), 6), p=p))\n",
        "  print ('Time elapsed:', datetime.datetime.now() - start_time)\n",
        "  \n",
        "# 5.3 Ниже приведены время работы и результаты обучения при разном количестве деревьев и показателе скорости обучения\n",
        "# В предложенных вариантах количества деревьев 10, 20, 30, оптимум однозначно не достигается, так как\n",
        "# при увеличении количества деревьев, показатель качества на кросс-валидации растет существенно\n",
        "# В своих расчетах я остановился на количестве деревьев 500, при скорости обучения 0.2, что соответсвует\n",
        "# разнице с предыдущим шагом меньше 0.001\n",
        "\n",
        "# Чтобы ускорить обучение при увеличении количества деревьев, можно:\n",
        "# 1) Брать не всю выборку, а какое-то случайное подмножество\n",
        "# 2) Можно упростить модель, например, уменьшить глубину деревьев (параметр max_depth)\n",
        "\n",
        "#n_estimators = 30, время обучения с кросс-валидацией ~ 1m25s\n",
        "#0.689949 learning_rate = 0.1\n",
        "#0.698246 learning_rate = 0.2\n",
        "#0.701622 learning_rate = 0.3\n",
        "#0.701574 learning_rate = 0.4\n",
        "#0.701795 learning_rate = 0.5 - максимум\n",
        "#0.700938 learning_rate = 0.6\n",
        "\n",
        "#n_estimators = 50, время обучения с кросс-валидацией ~ 2m20s\n",
        "#0.697816 learning_rate = 0.1\n",
        "#0.704457 learning_rate = 0.2\n",
        "\n",
        "#n_estimators = 100, время обучения с кросс-валидацией ~ 5m20s\n",
        "#0.712547 learning_rate = 0.2\n",
        "#0.713898 learning_rate = 0.3\n",
        "#0.71332  learning_rate = 0.4\n",
        "\n",
        "#n_estimators = 300, время обучения с кросс-валидацией ~ 16m\n",
        "#0.717853 learning_rate = 0.1\n",
        "#0.720567 learning_rate = 0.2\n",
        "#0.718898 learning_rate = 0.3\n",
        "\n",
        "#n_estimators = 400, время обучения с кросс-валидацией ~ 20m - похоже на 769 результат на kaggle (default gb)\n",
        "#0.721865 learning_rate = 0.2\n",
        "\n",
        "#n_estimators = 500, время обучения с кросс-валидацией ~ 24m30s - остановимся на этом шаге (разница с предыдущим меньше 0.001)\n",
        "#0.722444 learning_rate = 0.2\n",
        "\n",
        "#стоить заметить, что n_estimators и learning_rate связаны, и определяют\n",
        "#с какой скоростью мы будем двигаться в сторону антиградиента\n",
        "#можно зафиксировать один из параметров и варьировать другой\n",
        "\n",
        "#Проверим наш prediction на kaggle в late submission:\n",
        "#X_test = pd.read_csv('./features_test.csv', index_col='match_id')\n",
        "#X_test = X_test.fillna(300)\n",
        "#pred = pd.Series(clf.predict_proba(X_test)[:, 1], index=X_test.index, name = 'radiant_win')\n",
        "#X_test = pd.concat([X_test, pred], axis=1)\n",
        "#res = pd.Series(X_test['radiant_win'], name = 'radiant_win')\n",
        "#res.to_csv('./submission.csv')\n",
        "#грузим на kaggle, получаем score = 0.73219 на public leaderbord"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}